# HEVOcustomers
# PostgreSQL to Snowflake Pipeline with dbt

## Overview
This project demonstrates how to set up a data pipeline using PostgreSQL as the source and Snowflake as the destination, using Hevo for data ingestion and dbt for transformation.

## Prerequisites
Before you begin, ensure you have the following:
1. **Access to GitHub**
2. **Database knowledge (SQL)**
3. **dbt knowledge**
4. **Free trial of Snowflake**: [Sign up here](https://snowflake.com/free-trial/)
5. **Free trial of Hevo**: [Sign up here](https://hevo.com/)
6. **Self-hosted PostgreSQL instance**: Ensure PostgreSQL is running in a Docker container.
7. **Networking knowledge**: For connecting to your local database from Hevo.


## Steps to Build the Project

### 1. Install PostgreSQL
- Run a PostgreSQL instance using Docker:
  ```bash
  docker run --name postgres_container -e POSTGRES_PASSWORD=your_password -d -p 5432:5432 postgres



###**2. Access PostgreSQL**
Connect to your PostgreSQL instance. You can use any SQL client (like pgAdmin) or the psql command-line tool. If using psql, run:

bash
Copy code
docker exec -it postgres_container psql -U postgres

###**3. Create Tables**
Once connected, execute the following SQL commands to create the required tables:

sql
Copy code
CREATE TABLE raw_orders (
    id SERIAL PRIMARY KEY,
    user_id INT,
    order_date DATE,
    status VARCHAR(50)
);

CREATE TABLE raw_customers (
    id SERIAL PRIMARY KEY,
    first_name VARCHAR(100),
    last_name VARCHAR(100)
);

CREATE TABLE raw_payments (
    id SERIAL PRIMARY KEY,
    order_id INT,
    payment_method VARCHAR(50),
    amount DECIMAL(10, 2)
);
###**4. Load CSV Data**
You can load your CSV files into these tables. If you have the CSV files locally, you can use the COPY command. For example:

sql
Copy code
COPY raw_orders (user_id, order_date, status)
FROM '/path/to/your/raw_orders.csv' DELIMITER ',' CSV HEADER;

COPY raw_customers (first_name, last_name)
FROM '/path/to/your/raw_customers.csv' DELIMITER ',' CSV HEADER;

COPY raw_payments (order_id, payment_method, amount)
FROM '/path/to/your/raw_payments.csv' DELIMITER ',' CSV HEADER;
Make sure to adjust the path to where your CSV files are located.

###**5. Sign Up for Snowflake**
Visit the Snowflake sign-up page and create a free trial account. Note down your account credentials for later use.

###**6. Sign Up for Hevo**
Create a free trial account at Hevo. After signing up, familiarize yourself with the Hevo interface.

###**7. Set Up Hevo Pipeline**
Log in to your Hevo account and click on Create Pipeline.
For Source, select PostgreSQL and enter your database connection details (host, port, user, password).
For Destination, choose Snowflake and provide the necessary credentials.
Make sure to select Logical Replication as the ingestion mode during the setup.

###**8. Install dbt**
If you havenâ€™t installed dbt, do so using pip:

bash
Copy code
pip install dbt-snowflake

###**9. Initialize Your dbt Project**
Create a new dbt project:

bash
Copy code
dbt init your_project_name
Navigate to the project directory:

bash
Copy code
cd your_project_name

###**10. Create dbt Model**
In the models directory, create a new file called customers.sql and add the following SQL code:

sql
Copy code
WITH orders AS (
    SELECT user_id AS customer_id,
           MIN(order_date) AS first_order,
           MAX(order_date) AS most_recent_order,
           COUNT(id) AS number_of_orders
    FROM {{ ref('raw_orders') }}
    GROUP BY user_id
),
payments AS (
    SELECT order_id,
           SUM(amount) AS customer_lifetime_value
    FROM {{ ref('raw_payments') }}
    GROUP BY order_id
)
SELECT c.id AS customer_id,
       c.first_name,
       c.last_name,
       o.first_order,
       o.most_recent_order,
       o.number_of_orders,
       COALESCE(SUM(p.customer_lifetime_value), 0) AS customer_lifetime_value
FROM {{ ref('raw_customers') }} c
LEFT JOIN orders o ON c.id = o.customer_id
LEFT JOIN payments p ON o.id = p.order_id
GROUP BY c.id, c.first_name, c.last_name, o.first_order, o.most_recent_order, o.number_of_orders;
**11. Add dbt Tests**
In your tests directory, create a schema.yml file with the following content to validate your model:

yaml
Copy code
version: 2

models:
  - name: customers
    tests:
      - dbt_utils.not_null:
          column_name: customer_id
      - dbt_utils.unique:
          column_name: customer_id
###**12. Run dbt**
Execute the following commands to run your dbt models and tests:

bash
Copy code
dbt run
dbt test
**13. Push to GitHub**
Create a new repository on GitHub.
Initialize git in your project directory and push your code:
bash
Copy code
git init
git add .
git commit -m "Initial commit"
git remote add origin <your-repo-url>
git push -u origin master
Replace <your-repo-url> with the actual URL of your GitHub repository.
